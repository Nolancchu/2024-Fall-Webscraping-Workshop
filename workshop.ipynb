{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://quotes.toscrape.com/' # get the url of the site you want to scrape\n",
    "\n",
    "request = requests.get(url) # use the requests library to the http request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = bs4.BeautifulSoup(request.text, 'html.parser') \n",
    "# uncomment this if you want to see what beautifulsoup returns, it is rather long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_list = data.find_all('div', attrs={'class':'quote'}) # find_all finds every instance of the tag 'div', with the listed attributes\n",
    "#quote_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_data = { # establishing a dictionary in which to house the scrape data\n",
    "    'quote_text': [],\n",
    "    'author': [],\n",
    "    'tags': [],\n",
    "}\n",
    "\n",
    "for quote in quote_list:\n",
    "    \n",
    "    # for every quote in the quote list, we find the instance of the three things we're looking for, the quote text, the author, and the associated tags\n",
    "    # also note that .text gets rid of all the html stuff around the text\n",
    "    \n",
    "    quote_data['quote_text'].append(quote.find('span', attrs={'class':'text'}).text)\n",
    "    quote_data['author'].append(quote.find('small', attrs={'class':'author'}).text)\n",
    "    tags = [x.text for x in quote.find_all('a', attrs={'class':'tag'})]\n",
    "    quote_data['tags'].append(tags)\n",
    "    \n",
    "quote_df = pd.DataFrame(data = quote_data, columns = quote_data.keys())\n",
    "quote_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing pages manually for static sites\n",
    "\n",
    "quote_data = { # establishing a dictionary in which to house the scrape data\n",
    "    'quote_text': [],\n",
    "    'author': [],\n",
    "    'tags': [],\n",
    "}\n",
    "\n",
    "for i in range(0,11): # this workds because we know the way the url works, in terms of what happens when you change the page\n",
    "    url = f'https://quotes.toscrape.com/page/{i}/' \n",
    "    request = requests.get(url) \n",
    "    data = bs4.BeautifulSoup(request.text, 'html.parser') \n",
    "    quote_list = data.find_all('div', attrs={'class':'quote'})\n",
    "    \n",
    "    for quote in quote_list:\n",
    "    \n",
    "    # for every quote in the quote list, we find the instance of the three things we're looking for, the quote text, the author, and the associated tags\n",
    "    # also note that .text gets rid of all the html stuff around the text\n",
    "    \n",
    "        quote_data['quote_text'].append(quote.find('span', attrs={'class':'text'}).text)\n",
    "        quote_data['author'].append(quote.find('small', attrs={'class':'author'}).text)\n",
    "        tags = [x.text for x in quote.find_all('a', attrs={'class':'tag'})]\n",
    "        quote_data['tags'].append(tags)\n",
    "    \n",
    "quote_df_two = pd.DataFrame(data = quote_data, columns = quote_data.keys())\n",
    "quote_df_two\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super short example about automated webscraping with Selenium, chromedriver, and (still) beautiful soup\n",
    "\n",
    "driver = webdriver.Chrome() # establishing the chromedriver\n",
    "url = 'https://www.nba.com/stats/players/advanced' # getting our starting point url\n",
    "driver.get(url) # giving the url to our webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nba.com is a dynamic site, clicking around won't neccesarily change the url\n",
    "\n",
    "# we need to change the site to display all the players at once, which requires us to change the dynamic parts of the page\n",
    "select = Select(driver.find_element('xpath',r\"/html/body/div[1]/div[2]/div[2]/div[3]/section[2]/div/div[2]/div[2]/div[1]/div[3]/div/label/div/select\"))\n",
    "select.select_by_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statsDict = {\n",
    "    'NAME' : [],\n",
    "    'AGE' : [],\n",
    "    'TEAM' : [],\n",
    "    'GP' : [],\n",
    "    'W' : [],\n",
    "    'L' : [],\n",
    "    'MINUTES' : [],\n",
    "    'OFFRTG' : [],\n",
    "    'DEFRTG' : [],\n",
    "    'NETRTG' : [],\n",
    "    'AST%' : [],\n",
    "    'AST/TO' : [],\n",
    "    'AST RATIO' : [],\n",
    "    'OREB%' : [],\n",
    "    'DREB%' : [],\n",
    "    'REB%' : [],\n",
    "    'TO RATIO' : [],\n",
    "    'EFG%' : [],\n",
    "    'TS%' : [],\n",
    "    'USG%' : [],\n",
    "    'PACE' : [],\n",
    "    'PIE' : [], \n",
    "}\n",
    "\n",
    "src = driver.page_source\n",
    "parser = bs4.BeautifulSoup(src, 'html.parser') # establishing beautiful soup as our parser\n",
    "\n",
    "# finding the table \n",
    "table = parser.find('table', attrs={'class': 'Crom_table__p1iZz'}) \n",
    "tt = table.find('tbody', attrs={'class':'Crom_body__UYOcU'})\n",
    "\n",
    "# finding the player data\n",
    "players = tt.find_all('tr')[:20]\n",
    "playerList = [p.find_all('td') for p in players]\n",
    "\n",
    "# we want to append all of the data into our dictionary\n",
    "for player in playerList:\n",
    "    statsDict['NAME'].append(player[1])\n",
    "    statsDict['TEAM'].append(player[2])\n",
    "    statsDict['AGE'].append(player[3])\n",
    "    statsDict['GP'].append(player[4])\n",
    "    statsDict['W'].append(player[5])\n",
    "    statsDict['L'].append(player[6])\n",
    "    statsDict['MINUTES'].append(player[7])\n",
    "    statsDict['OFFRTG'].append(player[8])\n",
    "    statsDict['DEFRTG'].append(player[9])\n",
    "    statsDict['NETRTG'].append(player[10])\n",
    "    statsDict['AST%'].append(player[11])\n",
    "    statsDict['AST/TO'].append(player[12])\n",
    "    statsDict['AST RATIO'].append(player[13])\n",
    "    statsDict['OREB%'].append(player[14])\n",
    "    statsDict['DREB%'].append(player[15])\n",
    "    statsDict['REB%'].append(player[16])\n",
    "    statsDict['TO RATIO'].append(player[17])\n",
    "    statsDict['EFG%'].append(player[18])\n",
    "    statsDict['TS%'].append(player[19])\n",
    "    statsDict['USG%'].append(player[20])\n",
    "    statsDict['PACE'].append(player[21])\n",
    "    statsDict['PIE'].append(player[22])\n",
    "    \n",
    "# creating a dataframe with our scraped data\n",
    "player_df_dirty = pd.DataFrame(data = statsDict, columns= statsDict.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining functions in order to clean our dataframe \n",
    "\n",
    "def extract_name(html_string):\n",
    "    match = re.search(r'>([^<>]+)<', html_string)\n",
    "    return match.group(1) if match else html_string\n",
    "\n",
    "def clean_html_columns(df, columns):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).apply(extract_name)\n",
    "    return df\n",
    "\n",
    "player_df = clean_html_columns(player_df_dirty, player_df_dirty.columns)\n",
    "player_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive section:\n",
    "# We want to find the FIRST QUARTER statistics for Austin Reaves in the game on November 10th, using Selenium and Chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome() \n",
    "url = 'https://www.nba.com/stats/player/1630559/boxscores-traditional?Season=2023-24' \n",
    "driver.get(url) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to expand the advanced filters selector, so we'll need the xpath for it\n",
    "\n",
    "button = driver.find_element('xpath',r\"How do we find the xpath\\?\")\n",
    "button.click() # this line just clicks the button you have selected by the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to select the dropdown menu for the quarter box\n",
    "\n",
    "QuarterSelect = Select(driver.find_element('xpath',r\"How do we find the xpath\\?\"))\n",
    "QuarterSelect.select_by_index('What index should you select?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we want to confirm our filter with the website by selecting and clicking the large 'Get Stats button'\n",
    "\n",
    "getStats = driver.find_element('xpath',r\"How do we find the xpath\\?\")\n",
    "getStats.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like in the example, we have to have the website display all available games instead of the first 50 to display the game on November 11th\n",
    "\n",
    "AllGamesSelect = Select(driver.find_element('xpath',r\"How do we find the xpath\\?\"))\n",
    "AllGamesSelect.select_by_index(0) # pre-entered this one because it was acting strange during testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this it's already all written out just trust me\n",
    "\n",
    "src = driver.page_source\n",
    "parser = bs4.BeautifulSoup(src, 'html.parser')\n",
    "table = parser.find('table', attrs={'class': 'Crom_table__p1iZz'}) \n",
    "tt = table.find('tbody', attrs={'class':'Crom_body__UYOcU'})\n",
    "game = tt.find_all('tr')[73]\n",
    "N10game = []\n",
    "for i in game:\n",
    "    N10game.append(i.text)\n",
    "    \n",
    "N10gameDF = pd.DataFrame(data = N10game )\n",
    "N10gameDF = N10gameDF.T\n",
    "N10gameDF.columns = ['Match Up','W/L','MIN','PTS','FGM','FGA','FG%','3PM','3PA','3P%','FTM','FTA','FT%','OREB'\t,'DREB','REB','AST','STL','BLK','TOV','PF','+/-']\n",
    "N10gameDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should return true for all columns\n",
    "\n",
    "N10gameDF.iloc[0] == ['Nov 10, 2023 - LAL @ PHX','W','6:30','3','1','3','33.3','0','1','0.0','1','1','100','0','0','0','0','1','0','0','0','-7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to close the webdriver\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
